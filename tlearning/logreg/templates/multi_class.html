{% extends 'base.html' %}
<div id="main">

    {% block content %}
    <h1> {% block title %} The logistic regression {% endblock %}</h1>
    <div id="Sidebar" class="sidenav">
        <a href="javascript:void(0)" class="closebtn" onclick="closeNav()" id="closewithin">×</a>
        <a href="#multi">1. Logistic regression with multiple classes</a><br />
        <a style="margin-left: 2rem;" href="#model">1.1 Making the model</a><br />
        <a style="margin-left: 2rem;" href="#loss">1.2 Cross entropy loss</a><br />
        <a href="#test">2. Try it yourself</a><br />
        <a href="#softmax">3. The trick with the softmax function </a><br />



    </div>
    {% block head %}
    <!-- disable caching for THIS PAGE ONLY !-->
    <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
    <meta http-equiv="Pragma" content="no-cache" />
    <meta http-equiv="Expires" content="0" />

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: {
           equationNumbers: {  autoNumber: "AMS"  },
           extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
        }
      });
  </script>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    {% endblock %}
    <button class="openbtn" onclick="closeNav()" id="close">☰ Contents</button>
    <button class="openbtn" onclick="openNav()" id="open">☰ Contents</button>

    <h2 id="multi">Logistic regression with multiple classes</h2>
    <h3 id="model">Making the model</h3>
    <p align="justify">
        The thing is, binary classification, in \(\{\pm 1\}\) or \(\{0,1\}\), is quite restrictive.
        In many applications it is enough (dead/alive, animal/machine, safe/dangerous, etc). However, it can also
        be very useful to classify more than two categories, for example dog/cat/sheep/\(\dots\).

        The main tool for the binary classification was the sigmoid function. Shouldn't it be great to adapt
        that for more categories? Well, that's precisely what the \(\mathrm{softmax}\) function does. Let's say we have
        \(C\) classes.
        This function, also noted \(\sigma\) is defined as:
        $$\sigma(u)=\left(\frac{e^{u_1}}{\sum_{j=1}^C e^{u_j}},\dots,\frac{e^{u_C}}{\sum_{j=1}^C
        e^{u_j}}, \right)\enspace, $$
        with \(u\in\mathbb{R}^C\).

        The softmax function takes a vector in \(\mathbb{R}^C\) and returns another vector of the same size.
        However, thanks to the normalization by the sum, the output is actually a distribution. And in the logistic
        regression, we use it as the probability a posteriori to be in class \(c\in \{1,\dots, C\}\). So the model
        is now:

        $$\mathbb{P}(Y=c\,|\, X=x)=\frac{e^{\beta_c'x}}{\sum_{k=1}^C e^{\beta_k'x}} \enspace.$$

        What does this mean in practice? For a dataset of \(p\) features, the goal is now to estimate \(p+1\) parameters
        for each class.
        This means a total of \(C(p+1)\) parameters to estimate through the optimization method of your choice.
        This can quickly be a lot (quite a lot). Fortunatly for us, we are looking for a distribution. The sum of
        the outputs must be equal to \(1\). Thus, by estimating the probability to belong in all the \(C-1\) categories,
        we know that the last probability is simply \(1-\sum_{c=1}^{C-1} \mathbb{P}(Y=c\,|\, X)\). So when
        doing this approach called one-vs-all, we actually only need to estimate \((C-1)\times(p+1)\) parameters.

    </p>

    <h3 id="loss">Cross entropy loss</h3>
    <p align="justify">
    We have the model, but how do we train it? More precisely, how can we adapt the binary cross entropy loss to a non-binary situation.
    Well in term of terms, we simply use the cross entropy loss (pretty easy, just remove the binary). In term of maths, let \(\hat p\) be
    our estimation and \(p\) the vector of ones in the class \(c\) and zero elsewhere. Then our loss writes:

    $$ H(p, \hat p) = - \sum_{i=1}^C p_i \log(\hat{p}_i) = -\log(\hat{p}_c) \enspace .$$

    Here, we strongly use the convention \(0\log(0)=0\) by considering the limit and not the function evaluated at \(0\).
    </p>
    <p align="justify">
    Note that just as in the binary case, minimizing the loss is the same as maximizing the log-likelihood.
    And one more thing, this loss is convex, but not strictly convex. And we don't have the strong part because of the over-parametrization.
    Using a regularization like a \(l_2\) penalty lets us recover the strict convexity.
    </p>


    <h2 id="test">Try it yourself</h2>
    Here, we trained a logistic regression on the dataset CIFAR10.
    There are \(10\) labels and an equilibrated number of samples for each.
    The samples are images of \(32\times 32\) pixels, coded in RGB. Meaning that there are \(3\times 32^2 + 1\)
    parameters to
    adjust in this model. The goal for you is to simply write a number. The model will predict a label for the
    corresponding image
    from the test set. Try a few and admire the quality of the images. Note that the overall accuracy for the test set
    is around \(35\%\). So it is totally normal for the model to be wrong.

    <br>
    <div class="container">
        <div class="row h-200">
            <div class="col my-auto">

                <div>
                    <form action="/logreg/multi_class" method="POST">
                        <p>
                            The number entered should be between 0 and 9999. If it is not, the default value is 1 with
                            an image that does not correspond.
                        </p>
                        <br>
                        <label for="num">Image number</label>
                        <input type="text" id="num" name="num">
                        <br>
                        <input type="submit" value="Submit">
                    </form>
                </div>
            </div>
            <div class="col-8">
                <br>
                The predicted label is {{ prediction }} for index {{ index_ }}.
                <div id="bck-img"></div>
                <script type="text/javascript">
                    var container = document.getElementById("bck-img");
                    var image = document.createElement("img");
                    image.setAttribute("src", "{{url_for('logreg_bp.static', filename='cifar_img.png')}}?rand=" + new Date().getTime());
                    container.appendChild(image);
                </script>

            </div>
        </div>
    </div>

    <h2 id="softmax">The trick with the softmax function </h2>
    <p align="justify">
        If you've ever manipulated the exponential function, the issue computationnaly is that the overflow/underflow is
        never very far away. So using the softmax is quite a risk as we divide by a sum of exponential terms.
        A way to overcome this issue is to use the log-sum-exp function, noted \(LSE\) for short.

        The \(LSE\) function is simply: \(LSE(x)=\log(\sum_{i=1}^n e^{x_i})\). It's link with the softmax function is
        quite
        direct as:

        $$\frac{\partial LSE}{\partial x_{i_0}}(x)=\frac{e^{x_{i_0}}}{\sum_{i=1}^n e^{x_i}}=\sigma(x)_{i_0} \enspace.$$

        But it's not the only link, and the one that is the most interesting for us is the following:

        $$\log(\sigma(x)_j)=x_j - LSE(x) \enspace.$$

        Why is this so important? Recall that for any real \(b\), we have \(e^x=e^{x-b}e^b\). So it can be applied to
        the function log-sum-exp,
        \(LSE(x)=b + LSE(x-b).\) The choice of \(b=\max_{i} x_i\) lets us avoid the computational issues for the
        overflow, as doing so
        makes us compute at most the value for \(e^0=1\). This is useful when we want to compute the logarithm of the
        probability, but
        what about the probability itself? Because surely, if we have to do another exponentiation after it might ruin
        all the work.
        Well, the softmax function is defined with exponentials, so we can use the exponential function properties:

        $$
        \begin{aligned}
        \sigma(x)_j & = \frac{e^{x_j}}{\sum_i e^{x_i}} = \frac{e^b e^{x_j-b}}{\sum_i e^b e^{x_i - b}} \\
        & = \frac{e^{x_j - b}}{\sum_i e^{x_i -b}} \enspace,
        \end{aligned}
        $$

        so once again, taking \(b=\max_i x_i\) prevents any overflow in the computation.
        This property of the softmax function is called invariance by translation.
    </p>
    <a href="classification" class="previous">&laquo; Previous</a>
    <a href="#" class="next">Next &raquo;</a>

</div>


{% endblock %}